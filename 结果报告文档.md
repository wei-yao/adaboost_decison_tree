#机器学习实验结果报告文档

###魏尧   SA14006146

---

###实验内容

实现一个或多个分类器，例如KNN, 朴素贝叶斯, 决策树, 支持向量机, 神经网络等, 使用 Iris数据集进行测试。

可选(加分项): 实现一个集成方法（推荐Adaboost算法），并将其与基本分类器比较。

---

###实验方法


1.***数据预处理***

为了防止原数据相同类的数据集中在附近行对使用的检验方法影响，先对数据进行了随机置乱，并将符号属性替换为数字表示.
除了iris数据集，我还使用了glass和tic-tac-toe 数据集,预处理过的数据集以.mat文件保存.

---
2.***分类器***

我实现了knn，nativeBayes,decisionTree,support vector machine.

也实现了AdaBoost算法，其中基分类器使用decisonTree，迭代轮数为50.

各文件对应的函数分别如下：

方法 		  |函数名|
------------- | ---------|
 knn 		  |	customKnnClassify.m
 nativeBayes  |	customNativeBayes.m
 decisionTree | customDecisionTree.m
 svm		  |		customSvmClassify.m
 AdaBoost	  |	adaBoostExample.m

---

***检验方法***

使用了k折交叉验证.即将数据平均分成k份，每次选取其中一份做检验集，其他数据做训练集，这样每个数据都会被选作为检验集一次.

参数k选作10.

---

###实验结果

方法 		  | iris	 |glass |tic-tac-toe|
------------- | ---------|------|---------
knn  		  |  95.33  | 85.30 | 84.54
native bayes  | 95.94	| * 	| 71.70
decison tree  |  96.00 |  98.57	| 83.70
svm           | 97.33  |  98.72	| 86.92
AdaBoost	  | 94.00  |  98.57	| 99.89

*: glass 数据集不适合用 native bayes方法，所以此栏为空.

可以看到AdaBoost方法只有在 tic-tac-toe数据集上表现比较好.

在iris和glass数据集上结果不如基分类器使用的决策树好.说明集成方法的效果不一定好于基分类器.

---

###总结

我实现了 knn ，nativeBayes,decisionTree,support vector machine 方法，并实现了集成分类器 AdaBoost.

用10折交叉检验的方法 检验了各方法在 iris,glass,tic-tac-toe数据集上的性能.

学会了利用matlab的库实现分类器，收获很大.








